{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Conversation Buffer Memory\n",
    "- 지금까지의 대화 내용 전체를 저장하는 메모리\n",
    "- 대화 내용이 길어질 수록 메모리가 계속해서 커지기 때문에 비효율적\n",
    "    - 이전의 대화 내용 전체를 프롬프트로 AI에게 보내기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\":\"Hi\"}\n",
    "    , {\"output\":\"How are you?\"}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# Save_context를 반복 실행할 수록 메모리에 저장되는 데이터가 늘어남\n",
    "# Load_memory_variables를 통해 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Conversation Buffer Window Memory\n",
    "- Conversation Buffer Memory 중 (Window에 들어온) 일부만을 저장하는 메모리\n",
    "    - 예를 들어, 최근 5 개의 메시지를 저장하는 방식이면 6 번째 메시지가 들어오면 최초의 메시지는 지워짐\n",
    "- 모든 대화를 저장하지 않아도 되는 장점이 있음\n",
    "- 챗봇이 최근 대화에만 집중하는 단점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory_window = ConversationBufferWindowMemory(return_messages=True\n",
    "                                               , k=5 # 최근 5개의 데이터만 유지, 또는 window_size=5 로도 가능\n",
    "                                               )\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory_window.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory_window.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='0'), AIMessage(content='0')]}\n",
      "{'history': [HumanMessage(content='0'), AIMessage(content='0'), HumanMessage(content='1'), AIMessage(content='1')]}\n",
      "{'history': [HumanMessage(content='0'), AIMessage(content='0'), HumanMessage(content='1'), AIMessage(content='1'), HumanMessage(content='2'), AIMessage(content='2')]}\n",
      "{'history': [HumanMessage(content='0'), AIMessage(content='0'), HumanMessage(content='1'), AIMessage(content='1'), HumanMessage(content='2'), AIMessage(content='2'), HumanMessage(content='3'), AIMessage(content='3')]}\n",
      "{'history': [HumanMessage(content='0'), AIMessage(content='0'), HumanMessage(content='1'), AIMessage(content='1'), HumanMessage(content='2'), AIMessage(content='2'), HumanMessage(content='3'), AIMessage(content='3'), HumanMessage(content='4'), AIMessage(content='4')]}\n",
      "{'history': [HumanMessage(content='1'), AIMessage(content='1'), HumanMessage(content='2'), AIMessage(content='2'), HumanMessage(content='3'), AIMessage(content='3'), HumanMessage(content='4'), AIMessage(content='4'), HumanMessage(content='5'), AIMessage(content='5')]}\n",
      "{'history': [HumanMessage(content='2'), AIMessage(content='2'), HumanMessage(content='3'), AIMessage(content='3'), HumanMessage(content='4'), AIMessage(content='4'), HumanMessage(content='5'), AIMessage(content='5'), HumanMessage(content='6'), AIMessage(content='6')]}\n",
      "{'history': [HumanMessage(content='3'), AIMessage(content='3'), HumanMessage(content='4'), AIMessage(content='4'), HumanMessage(content='5'), AIMessage(content='5'), HumanMessage(content='6'), AIMessage(content='6'), HumanMessage(content='7'), AIMessage(content='7')]}\n",
      "{'history': [HumanMessage(content='4'), AIMessage(content='4'), HumanMessage(content='5'), AIMessage(content='5'), HumanMessage(content='6'), AIMessage(content='6'), HumanMessage(content='7'), AIMessage(content='7'), HumanMessage(content='8'), AIMessage(content='8')]}\n",
      "{'history': [HumanMessage(content='5'), AIMessage(content='5'), HumanMessage(content='6'), AIMessage(content='6'), HumanMessage(content='7'), AIMessage(content='7'), HumanMessage(content='8'), AIMessage(content='8'), HumanMessage(content='9'), AIMessage(content='9')]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    add_message(i,i)\n",
    "    print(get_history())\n",
    "\n",
    "# 최근 5개의 데이터만 유지되는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Conversation Summary Memory\n",
    "- llm을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory_summary = ConversationSummaryMemory(llm=llm, return_messages=True) \n",
    "# llm을 통해 memory를 사용하므로 비용이 발생함\n",
    "# ConversationSummrayMemory는 대화 내용을 요약하여 저장함\n",
    "# 짧은 대화에서는 초기 비용이 발생하지만, 대화가 길어질수록 비용이 줄어듦\n",
    "\n",
    "def add_message_(input, output):\n",
    "    memory_summary.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history_():\n",
    "    return memory_summary.load_memory_variables({})\n",
    "\n",
    "add_message_(\"Hi I'm Minseop Ji, I live in South Korea\", \"Hi Minseop, that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message_(\"South Korea is so pretty\", \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Minseop Ji from South Korea. The AI responds by saying that it thinks that is cool and expresses a desire to visit South Korea because it is so pretty.')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Conversation Summary Buffer Memory\n",
    "- Conversation Summary Memory와 Conversation Buffer Memory의 결합 모델\n",
    "- 메모리에 메시지의 개수를 저장하고, 저장 한도를 넘어설 때 기존의 메모리를 삭제하는 것 대신 Summary를 통해 요약 저장하여 보관"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory_summary_buffer = ConversationSummaryBufferMemory(llm=llm, return_messages=True, max_token_limit=50)\n",
    "# token limit을 50으로 설정하면, 50개의 토큰만 유지하고 나머지는 삭제함\n",
    "\n",
    "def add_message__(input, output):\n",
    "    memory_summary_buffer.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history__():\n",
    "    return memory_summary_buffer.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Minseop Ji, I live in South Korea\"),\n",
       "  AIMessage(content='Hi Minseop, that is so cool!')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message__(\"Hi I'm Minseop Ji, I live in South Korea\", \"Hi Minseop, that is so cool!\")\n",
    "get_history__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Minseop Ji from South Korea.'),\n",
       "  AIMessage(content='Hi Minseop, that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!')]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message__(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"The human introduces themselves as Minseop Ji from South Korea. The AI responds by saying it's cool and the human mentions that South Korea is pretty.\"),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I'm not sure, but I think it's pretty far\")]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message__(\"How far is Korea from Argentina?\", \"I'm not sure, but I think it's pretty far\")\n",
    "get_history__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SystemMessage로 요약되어 memory에 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Conversation Knowledge Graph Memory\n",
    "\n",
    "- 마찬가지로 llm을 사용하는 Memory\n",
    "- 대화 중에 Entity의 Knowledge Graph를 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory_kg = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message___(input, output):\n",
    "    memory_kg.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history___():\n",
    "    return memory_kg.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Minseop Ji: Minseop Ji lives in South Korea. Minseop Ji is a person.')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message___(\"Hi I'm Minseop Ji, I live in South Korea\", \"Hi Minseop, that is so cool!\")\n",
    "memory_kg.load_memory_variables({\"input\":\"Who is Minseop Ji?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 Entity (Minseop Ji)에 대한 성격을 정의하고 답변을 작성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Minseop Ji: Minseop Ji lives in South Korea. Minseop Ji is a person. Minseop Ji likes kimchi.')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message___(\"Minseop Ji likes kimchi.\", \"Minseop, that is so cool!\")\n",
    "memory_kg.load_memory_variables({\"input\":\"what does Minseop Ji like?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Chain To use Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### llm chain\n",
    "- off-the-shelf(일반 목적 chain)의 일종\n",
    "- 이미 만들어져 있는 chain인데 직접 모델을 만들 경우 부적합한 경우가 많음\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a helpful AI talking to a human.\n",
      "\n",
      "[]\n",
      "Human: My name is Minseop Ji.\n",
      "You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Minseop Ji! How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory_summary_buffer = ConversationSummaryBufferMemory(\n",
    "    llm=llm\n",
    "    , return_messages=True\n",
    "    , max_token_limit=80\n",
    "    , memory_key=\"chat_history\" # memory_key를 지정하면, 해당 key에 대한 prompt를 구성함\n",
    ")\n",
    "\n",
    "# 챗봇이 이전의 대화 내역을 기억하도록 prompt를 구성함\n",
    "template = \"\"\"\n",
    "You are a helpful AI talking to a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "You:\n",
    "\"\"\"\n",
    "\n",
    "# memory_summary_buffer를 LLMChain에 적용\n",
    "chain = LLMChain(\n",
    "    llm=llm\n",
    "    , memory=memory_summary_buffer\n",
    "    # , prompt = PromptTemplate.from_template(\"{question}\") 이렇게 구현할 경우 prompt에 history가 포함되지 않아 챗봇이 이전의 대화 내역을 기억하지 못함\n",
    "    , prompt = PromptTemplate.from_template(template)\n",
    "    , verbose=True # chain의 prompt log를 출력함\n",
    ")\n",
    "\n",
    "chain.predict(question = \"My name is Minseop Ji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a helpful AI talking to a human.\n",
      "\n",
      "[HumanMessage(content='My name is Minseop Ji.'), AIMessage(content='Hello Minseop Ji! How can I assist you today?')]\n",
      "Human: I live in Cheongju, South Korea.\n",
      "You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Cheongju is a beautiful city in South Korea. How can I assist you today, Minseop Ji?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"I live in Cheongju, South Korea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a helpful AI talking to a human.\n",
      "\n",
      "[SystemMessage(content='The human introduces themselves as Minseop Ji.'), AIMessage(content='Hello Minseop Ji! How can I assist you today?'), HumanMessage(content='I live in Cheongju, South Korea.'), AIMessage(content=\"That's great! Cheongju is a beautiful city in South Korea. How can I assist you today, Minseop Ji?\")]\n",
      "Human: What is my name?\n",
      "You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Minseop Ji.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"What is my name?\")\n",
    "# 이전에 입력한 대화 내용을 prompt에 반영하지 못해서 정확한 답변을 얻지 못함\n",
    "# 반면 load_memory_variables에는 정상적으로 입력한 대화 내용이 반영 및 요약되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Chat 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Minseop Ji.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Minseop Ji! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory_summary_buffer = ConversationSummaryBufferMemory(\n",
    "    llm=llm\n",
    "    , return_messages=True\n",
    "    , max_token_limit=80\n",
    "    , memory_key=\"chat_history\" # memory_key를 지정하면, 해당 key에 대한 prompt를 구성함\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\")\n",
    "    , MessagesPlaceholder(variable_name=\"chat_history\") # chat_history를 placeholder로 지정함\n",
    "    , (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# memory_summary_buffer를 LLMChain에 적용\n",
    "chain = LLMChain(\n",
    "    llm=llm\n",
    "    , memory=memory_summary_buffer\n",
    "    , prompt = prompt\n",
    "    , verbose=True # chain의 prompt log를 출력함\n",
    ")\n",
    "\n",
    "chain.predict(question = \"My name is Minseop Ji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Minseop Ji.\n",
      "AI: Hello Minseop Ji! How can I assist you today?\n",
      "Human: I live in Cheongju, South Korea.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Minseop Ji! Cheongju is a beautiful city in South Korea. Is there anything specific you would like to know or discuss about Cheongju?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"I live in Cheongju, South Korea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "System: The human introduces themselves as Minseop Ji.\n",
      "AI: Hello Minseop Ji! How can I assist you today?\n",
      "Human: I live in Cheongju, South Korea.\n",
      "AI: That's great to know, Minseop Ji! Cheongju is a beautiful city in South Korea. Is there anything specific you would like to know or discuss about Cheongju?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Minseop Ji.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Lang Chain Expression 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'My name is Minseop Ji.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Minseop Ji! How can I assist you today?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory_summary_buffer = ConversationSummaryBufferMemory(\n",
    "    llm=llm\n",
    "    , return_messages=True\n",
    "    , max_token_limit=80\n",
    "    , memory_key=\"chat_history\" # memory_key를 지정하면, 해당 key에 대한 prompt를 구성함\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\")\n",
    "    , MessagesPlaceholder(variable_name=\"chat_history\") # chat_history를 placeholder로 지정함\n",
    "    , (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory_summary_buffer.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "# chain = prompt | llm \n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm \n",
    "# 이렇게 구현하면 load_memory를 통해 chat_history를 먼저 가져와서 chat_history에 할당\n",
    "\n",
    "chain.invoke({\n",
    "    # \"chat_history\": memory.load_memory_variables({})[\"chat_history\"] <- 이렇게 구현할 경우 매 chain 호출마다 chat_history를 추가해야 함\n",
    "    \"question\": \"My name is Minseop Ji.\"\n",
    "})\n",
    "# load_memory에 input이 들어가고, load_memory의 출력이 chat_history에 할당됨\n",
    "# 이후 prompt에 chat_history가 입력되고, prompt의 출력이 llm에 입력됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
